{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment1_00.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the code for assignment 1 - Text Analytics. \n"
      ],
      "metadata": {
        "id": "oZQWBx6OxcdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reading Data"
      ],
      "metadata": {
        "id": "a4rgPrAfx00F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_data = open(\"pos.txt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PqCzMCtvx3nm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(positive_data.read())"
      ],
      "metadata": {
        "id": "vtB_JzJRypHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_data = open(\"neg.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "AMO0pw0FynqZ",
        "outputId": "cde89213-216e-4d08-8d6a-c1ae7396b77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bb9c1bebe870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnegative_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"neg.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'neg.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1 - Text Tokanization"
      ],
      "metadata": {
        "id": "xcDznfp2xinW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-Tsl-bVzQ8Q",
        "outputId": "3cfe0ae9-b3d0-404d-dc2b-c240b8e544fa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize text using split \n",
        "\n",
        "string_data = positive_data.read()\n",
        "\n",
        "token_pos_split = string_data.split()"
      ],
      "metadata": {
        "id": "xwOMToN3xgL_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2 - Special Characters Removal\n"
      ],
      "metadata": {
        "id": "tMOdbnguxnKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install string"
      ],
      "metadata": {
        "id": "0LmMSR-A1BV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install re"
      ],
      "metadata": {
        "id": "VIHwe94s2Eij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation\n",
        "\n",
        "\n",
        "def punc_remove(txt):\n",
        "  \"\"\"\n",
        "  function returns a string with no punctuation marks.\n",
        "  \"\"\"\n",
        "  noPunc = \"\".join([i for i in txt if i not in string.punctuation])\n",
        "\n",
        "  return noPunc\n",
        "\n",
        "#data_no_punc has no punctuation marks \n",
        "data_no_punc = punc_remove(string_data)\n",
        "\n",
        "#special characters removal\n",
        "import re\n",
        "\n",
        "data_no_spc_char = re.sub(r'[^\\w\\s]','', data_no_punc)\n"
      ],
      "metadata": {
        "id": "oI0UVLqixqo3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking \n",
        "print(data_no_spc_char.isalnum())\n"
      ],
      "metadata": {
        "id": "VwKdXo1W2XFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba2e0ca8-0f10-4a6c-c7d3-88eb97e79eab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_characters = \"!@#$%^&*()-+?_=,<>/\"\n",
        "\n",
        "if any(c in special_characters for c in data_no_spc_char):\n",
        "    print(\"yes\")\n",
        "else:\n",
        "    print(\"no\")\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph7raiuh2saS",
        "outputId": "e9644357-6f38-4636-bfaa-b4b96203fb00"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means the data is not alphanumeric (there are numbers in there) but there are no longer any special characters or puncuation marks. "
      ],
      "metadata": {
        "id": "_RO62DnQ3L-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3 - Dataset with and without Stop Words"
      ],
      "metadata": {
        "id": "ZdRpdrn_xrMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stop words \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsK3X4BYxvDm",
        "outputId": "968aa585-de7b-4787-9bb3-b81b34a43d5b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_eng = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "c69RU2bvz_U3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(txt):\n",
        "  no_stopwords = \"\".join([i for i in txt if i not in stop_words_eng])\n",
        "\n",
        "  return no_stopwords\n",
        "\n",
        "data_no_stopwords = remove_stopwords(data_no_spc_char)\n"
      ],
      "metadata": {
        "id": "HdS1SyHm0UC8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just sanity check \n",
        "\n",
        "print(\"length of string at first: \", len(string_data))\n",
        "print(\"length of string data at the end: \", len(data_no_stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxjgGgFs3yiw",
        "outputId": "69fd2c98-1184-4ca3-f93a-a207d2e6f8ec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of string at first:  1048576\n",
            "length of string data at the end:  645783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, there are now two versions of the data: \n",
        "\n",
        "data_no_stopwords : Data with no stopwords \n",
        "data_no_spc_char : data with stopwords "
      ],
      "metadata": {
        "id": "t4KMS1u64sOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#just to be able to keep track easier; I'll create new variables \n",
        "\n",
        "wstopword_data = data_no_spc_char\n",
        "wostopwords_data = data_no_stopwords"
      ],
      "metadata": {
        "id": "wDR02BN04zrQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4 - Data splitting into Train, Validation and Test set."
      ],
      "metadata": {
        "id": "3GIuDYZ_xvc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlp"
      ],
      "metadata": {
        "id": "HQ1bADcCxzNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9868b1-dffd-4007-867d-a95bc36880e7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nlp) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from nlp) (4.64.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 13.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from nlp) (3.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlp) (1.21.6)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from nlp) (0.3.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2022.5.18.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
            "Installing collected packages: xxhash, nlp\n",
            "Successfully installed nlp-0.4.0 xxhash-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nlp\n",
        "\n",
        "\n",
        "trn_tst_stopword_dict = nlp.Dataset.wstopword_data.train_test_split(test_size = 0.2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "N9B6Cp725dyR",
        "outputId": "1b574d56-f582-41e7-8db3-ea9268c7ab36"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-02d776cfda9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrn_tst_stopword_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwstopword_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Dataset' has no attribute 'wstopword_data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split datasets: \n",
        "\n",
        "wstopwords_tokenized = wstopword_data.split()\n",
        "wostopwords_tokenized = wostopwords_data.split()\n"
      ],
      "metadata": {
        "id": "qfax3m_D8_1m"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "import math \n",
        "\n",
        "\n",
        "\n",
        "shuf_with_stp = list(wstopword_data)\n",
        "shuf_without_stp = list(wostopwords_tokenized)\n",
        "\n",
        "shuffle(shuf_with_stp)\n",
        "shuffle(shuf_without_stp)\n",
        "\n",
        "def train_test(txt_data, n):\n",
        "  split = n\n",
        "  split_index = math.floor(len(txt_data) * split)\n",
        "  trainset = txt_data[:split_index]\n",
        "  testset = txt_data[split_index:]\n",
        "\n",
        "  return trainset, testset\n",
        "\n",
        "#training set \n",
        "data_token_wstops_train = train_test(shuf_with_stp, 0.8)[0]\n",
        "placeholder_for_test = train_test(shuf_with_stp, 0.8)[1]\n",
        "\n",
        "#test and validation set\n",
        "data_token_wstops_test = train_test(placeholder_for_test, 0.5)[0]\n",
        "data_token_wstops_validation = train_test(placeholder_for_test, 0.5)[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "_DgjjWOl8yso"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#repeat the same for without stopwords \n",
        "#training set \n",
        "data_token_wostops_train = train_test(shuf_without_stp, 0.8)[0]\n",
        "placeholder_for_test2 = train_test(shuf_without_stp, 0.8)[1]\n",
        "\n",
        "#test and validation set\n",
        "data_token_wostops_test = train_test(placeholder_for_test2, 0.5)[0]\n",
        "data_token_wostops_validation = train_test(placeholder_for_test2, 0.5)[1]"
      ],
      "metadata": {
        "id": "RnXGnS1U6aUo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLOSING"
      ],
      "metadata": {
        "id": "7gqorOa55eiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#very last cell \n",
        "\n",
        "positive_data.close()\n",
        "# negative_data.close()"
      ],
      "metadata": {
        "id": "3u3CnPCqyzW5"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TiWFwkppB1ac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}